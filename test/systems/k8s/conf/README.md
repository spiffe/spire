# A minimal SPIRE deployment in Kubernetes

## Introduction

This guide will walkthrough getting SPIRE server and agent running in a
Kubernetes cluster. Note that no application or workload is configured with
this example - it is purely setting up the SPIRE server and agent infrastructure
in Kubernetes with node attestation.

In order to follow this guide, you'll need a working Kubernetes cluster.
Minikube is just fine - in fact the example .yaml files assume minikube and will
have to be modified in one place otherwise (the change is noted in the guide).

## General design

+ All SPIRE components will be created in a namespace called **spire**. Note
  that most kubectl commands will need to be run with "--namespace spire".
+ The SPIRE server will run in a service account named **spire-server** and the
  agent in a **spire-agent** service account.
+ The SPIRE server deployment runs on the **master** Kubernetes node.
+ The SPIRE agent daemonset runs on all Kubernetes worker nodes.
+ The SPIRE server uses a hostPath bind mount for persisting it's keys and
  sqlite database. In a production deployment, a more robust and secure
  persistence mechanism is required. This configuration is outside the scope of
  this example.
+ The SPIRE agent uses a hostPath bind mount for sharing the agent API's UNIX
  domain socket with application containers.
+ The dummy bootstrap CA from the SPIRE source tree is used. In a production
  deployment, this needs to be replaced with a proper secure CA using short
  expiration and certificate rotation.
+ A recent version of Kubernetes - we've tested 1.13.1, 1.12.4, and 1.10.12.

## A. Configure SPIRE namespace

### A1. Apply spire namespace

Take a look at [spire-namespace.yaml](spire-namespace.yaml), which will create
the **spire** namespace that all components will be deployed in.

Apply this configuration:

```
$ kubectl apply -f spire-namespace.yaml
namespace/spire created
```

You should see **spire** listed in the output of "kubectl get namespaces".

## B. Configure SPIRE server

### B1. Create server service account

Next, we'll apply [server-account.yaml](server-account.yaml), which will
configure a service account named **spire-server** in the **spire** namespace:

```
$ kubectl apply -f server-account.yaml
serviceaccount/spire-server created
```

### B2. Create server secrets

This example is using the dummy bootstrap CA, which is configured via
[server-secrets.yaml](server-secrets.yaml). As mentioned, in a production
environment this needs to be replaced with a more secure configuration and
certificate rotation.

The values for **upstream_ca.pem** and **upstream_ca.key** are base64 encodings
of the files that can be found
[here in the spire source tree](https://github.com/spiffe/spire/tree/master/conf/server).
For reference, these values were generated by running the following base64
command for each file:

```
$ base64 -w0 spire/conf/server/dummy_upstream_ca.crt
```

When applied, this will create a set of secrets names **spire-server** in the
**spire** namespace:

```
$ kubectl apply -f server-secrets.yaml
secret/spire-server created
```

### B3. Create server configmap

The configuration for the SPIRE server is handled via a Kubernetes configmap in
[server-configmap.yaml](server-configmap.yaml). You'll note several directories
such as /run/spire/data, /run/spire/secrets, and /run/k8s-certs which will be
bound in during the next step when the server container is deployed.

**NOTE: When running in a real environment, the cluster name in the k8s_sat
NodeAttestor should be the name of your cluster, instead of "demo-cluster",
and must match the same cluster name provided in agent-configmap.yaml.**

```
$ kubectl apply -f server-configmap.yaml
configmap/spire-server created
```

This will apply a configmap named **spire-server** in the **spire** namespace.

### B4. Create server deployment

Now, we can deploy the SPIRE server by applying the configuration
[server-deployment.yaml](server-deployment.yaml). This binds in several
volumes:

+ **spire-config** - this is a reference to the **spire-server** configmap from
  the previous step.
+ **spire-secrets** - this is a reference to the **spire-server** secrets from
  step B2.
+ **spire-data** - this is a hostPath for the server's sqlite database and
  keys file. As mentioned above, a production deployment should use a much
  more secure and robust persistence mechanism.
+ **k8s-sa-cert** - this is the public key used for validating service accounts
  and must be the exact same file passed via **--service-account-key-file** to
  kube-apiserver. This will be mounted at **/run/k8s-certs/<file>/**.

**NOTE: When not running in minikube, change the path for k8s-sa-cert from
"/var/lib/minikube/certs/sa.pub" to the path of the service account public key
passed to kube-apiserver. If the argument --service-account-key-file is not
passed, Kubernetes will use the value of --tls-private-key-file instead, and
SPIRE will need to be configured to use the certificate for that key, which
should be the value of --tls-cert-file. Describe the pod for "kube-apiserver"
in the "kube-system" namespace to find these values for a given cluster.
It is only necessary to change the hostPath entry - the mountPath of
"/run/k8s-certs/sa.pub" does not need to change (but if you do, you'll need
to make the same change in "server-configmap.yaml" and reapply it).**

The server deployment uses a nodeSelector to insure that it runs on a "master"
node. When running in an environment without an accessible "master", such as
EKS, a node for the SPIRE server will need to be selected and provisioned, and
the nodeSelector in "server-deployment.yaml" will need to change.

The server deployment will also configure a livenessProbe on the SPIRE server's
GRPC port.

```
$ kubectl apply -f server-deployment.yaml
deployment.apps/spire-server created
```

This will create a deployment called **spire-server** in the **spire**
namespace and should start a spire-server pod as well:

```
$ kubectl get deployments --namespace spire
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
spire-server   1/1     1            1           86m

$ kubectl get pods --namespace spire
NAME                           READY   STATUS    RESTARTS   AGE
spire-server-b95945658-4wbkd   1/1     Running   0          86m
```

### B5. Create server service

The final step in deploying the SPIRE server is to create a service, which is
described in [server-service.yaml](server-service.yaml).

```
$ kubectl apply -f server-service.yaml
service/spire-server created
```

You should now see a **spire-server** service in the **spire** namespace:

```
$ kubectl get services --namespace spire
NAME           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
spire-server   NodePort   10.107.205.29   <none>        8081:30337/TCP   88m
```

### B6. Examine SPIRE server logs

To see the SPIRE server logs, get the name of the pod as shown above in the
deployment step, then we tail the pod's log:

```
$ kubectl logs -f spire-server-b95945658-4wbkd --namespace spire
time="2019-01-07T20:41:39Z" level=debug msg="Setting umask to 077"
time="2019-01-07T20:41:39Z" level=info msg="data directory: \"/run/spire/data\""
time="2019-01-07T20:41:39Z" level=info msg="Starting plugin catalog" subsystem_name=catalog
time="2019-01-07T20:41:39Z" level=debug msg="DataStore(sql): configuring plugin" subsystem_name=catalog
time="2019-01-07T20:41:39Z" level=debug msg="NodeAttestor(k8s_sat): configuring plugin" subsystem_name=catalog
time="2019-01-07T20:41:39Z" level=debug msg="NodeResolver(noop): configuring plugin" subsystem_name=catalog
time="2019-01-07T20:41:39Z" level=debug msg="KeyManager(disk): configuring plugin" subsystem_name=catalog
time="2019-01-07T20:41:39Z" level=debug msg="UpstreamCA(disk): configuring plugin" subsystem_name=catalog
time="2019-01-07T20:41:39Z" level=info msg="plugins started"
time="2019-01-07T20:41:39Z" level=debug msg="Loaded keypair set \"A\"" subsystem_name=ca_manager
time="2019-01-07T20:41:39Z" level=debug msg="Loaded keypair set \"B\"" subsystem_name=ca_manager
time="2019-01-07T20:41:39Z" level=debug msg="Loaded keypair sets" subsystem_name=ca_manager
time="2019-01-07T20:41:39Z" level=debug msg="Activating keypair set \"B\"" subsystem_name=ca_manager
```

## C. Configure SPIRE agent

### C1. Create agent service account

As with the SPIRE server, we'll apply [agent-account.yaml](agent-account.yaml),
which will configure a service account named **spire-agent** in the **spire**
namespace.:

```
$ kubectl apply -f agent-account.yaml
serviceaccount/spire-agent created
```

### C2. Create agent secrets

This example is using the dummy root CA, which is configured via
[agent-secrets.yaml](agent-secrets.yaml). The same comments apply as for the
SPIRE server regarding security and how this base64 string was generated.

When applied, this will create a set of secrets names **spire-agent** in the
**spire** namespace:

```
$ kubectl apply -f agent-secrets.yaml
secret/spire-agent created
```

### C3. Create agent configmap

The configuration for the SPIRE agent is in the
[agent-configmap.yaml](agent-configmap.yaml) configmap. You'll note
/run/spire/secrets and /run/spire/sockets which we'll bind
during the next step, similar to the server but a slightly different set.

**NOTE: When running in a real environment, the cluster name in the k8s_sat
NodeAttestor should be the name of your cluster, instead of "demo-cluster",
and must match the same cluster name provided in server-configmap.yaml.**

```
$ kubectl apply -f agent-configmap.yaml
configmap/spire-agent created
```

This will apply a configmap named **spire-agent** in the **spire** namespace.

### C4. Create agent daemonset

The SPIRE agent is deployed as a daemonset and will run on each Kubernetes
worker. We deploy the SPIRE agent by applying the configuration
[agent-daemonset.yaml](agent-daemonset.yaml). This binds in the following
volumes:

+ **spire-config** - this is a reference to the **spire-agent** configmap from
  the previous step.
+ **spire-secrets** - this is a reference to the **spire-agent** secrets from
  step C2.
+ **spire-sockets** - this is a hostPath which will be shared with all other
  pods running on the same worker host. This contains a UNIX domain socket that
  workloads use to communicate with the SPIRE agent API.

The daemonset configuration will configure a livenessProbe that test's the
agent's API socket (currently this is done by abusing JWT token validation,
but we are working on a better healthcheck mechanism).

```
$ kubectl apply -f agent-daemonset.yaml
daemonset.apps/spire-agent created
```

This will create a daemonset called **spire-agent** in the **spire**
namespace and should start a spire-agent pod along side spire-server:

```
$ kubectl get daemonset --namespace spire                                                                                  master
NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
spire-agent   1         1         1       1            1           <none>          6m45s

$ kubectl get pods --namespace spire
NAME                           READY   STATUS    RESTARTS   AGE
spire-agent-88cpl              1/1     Running   0          6m45s
spire-server-b95945658-4wbkd   1/1     Running   0          103m
```

### C5. Examine SPIRE agent logs

As with the server, obtain the running agent pod and use the kubectl logs
command to tail the agent logs:

```
$ kubectl logs -f spire-agent-88cpl --namespace spire
time="2019-01-07T22:20:59Z" level=info msg="Starting plugin catalog" subsystem_name=catalog
time="2019-01-07T22:20:59Z" level=debug msg="WorkloadAttestor(k8s): configuring plugin" subsystem_name=catalog
time="2019-01-07T22:20:59Z" level=debug msg="WorkloadAttestor(unix): configuring plugin" subsystem_name=catalog
time="2019-01-07T22:20:59Z" level=debug msg="NodeAttestor(k8s_sat): configuring plugin" subsystem_name=catalog
time="2019-01-07T22:20:59Z" level=debug msg="KeyManager(memory): configuring plugin" subsystem_name=catalog
time="2019-01-07T22:20:59Z" level=debug msg="No pre-existing agent SVID found. Will perform node attestation" subsystem_name=attestor
time="2019-01-07T22:29:31Z" level=info msg="Starting workload API" subsystem_name=endpoints
```

You can verify the agent attested to the server by looking at the latest server
logs, which will contain output such as:

```
time="2019-01-07T23:49:13Z" level=debug msg="Signing CSR for Agent SVID spiffe://example.org/spire/agent/k8s_sat/minikube/139f5941-b83b-43f9-b35c-cafbe720d3ff" subsystem_name=node_api
time="2019-01-07T23:49:13Z" level=debug msg="Signed x509 SVID \"spiffe://example.org/spire/agent/k8s_sat/minikube/139f5941-b83b-43f9-b35c-cafbe720d3ff\" (expires 2019-01-07T23:59:39Z)" subsystem_name=ca_manager
time="2019-01-07T23:49:13Z" level=debug msg="could not find node resolver type %qk8s_sat" subsystem_name=node_api
time="2019-01-07T23:49:13Z" level=info msg="Node attestation request from 192.168.122.147:36718 completed using strategy k8s_sat" subsystem_name=node_api
```

## Example client deployment

A client container needs access to the agent API socket. An example on how
to configure that can be found in
[client-deployment.yaml](client-deployment.yaml) - note the **volumeMounts**
and **volumes** configuration stanzas to see how the UNIX domain socket
**agent.sock** is bound in.

You can test that the agent socket is accessible from an application container
by applying client-deployment.yaml, which is a no-op container using the
spire-k8s docker image used for the server and agent.

```
$ kubectl apply -f client-deployment.yaml
deployment.apps/client created

$ kubectl get pods
NAME                      READY   STATUS    RESTARTS   AGE
client-6f9659bd44-m98vv   1/1     Running   0          18s

$ kubectl exec -it client-6f9659bd44-m98vv /bin/bash
```

From this shell, we can verify the socket is functional by (ab)using the same
SPIRE agent API called used in the agent's livenessProbe:

```
root@minikube:/# /opt/spire/bin/spire-agent api validate jwt -audience foo -svid foo -socketPath /run/spire/sockets/agent.sock
SVID is not valid: token contains an invalid number of segments
```

That error message indicates that we were able to talk to the agent over the
API UNIX domain socket. If the agent is not running an error message such as
the following will be output:

```
Unable to validate JWT SVID: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = "transport: Error while dialing dial unix /run/spire/sockets/agent.sock: connect: connection refused"
```

Delete this container via:

```
$ kubectl delete deployment client
```

## Teardown all SPIRE components

To remove everything we just configured, run the following commands which
will delete all deployments and configurations for the agent, server, and
namespace:

```
$ kubectl delete daemonset spire-agent --namespace spire
$ kubectl delete configmap spire-agent --namespace spire
$ kubectl delete secrets spire-agent --namespace spire
$ kubectl delete serviceaccount spire-agent --namespace spire
$ kubectl delete deployment spire-server --namespace spire
$ kubectl delete service spire-server --namespace spire
$ kubectl delete configmap spire-server --namespace spire
$ kubectl delete secrets spire-server --namespace spire
$ kubectl delete serviceaccount spire-server --namespace spire
$ kubectl delete namespace spire
```
